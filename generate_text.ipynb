{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045f08bb-e41a-420b-92f4-34d55976e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([{'id': 0,\n",
    "  'library': 'numPy',\n",
    "  'deprecated': 'numpy.array()',\n",
    "  'updated': 'numpy.matrixList()',\n",
    "  'prompt': 'How can you create a numpy array from a list [1, 2, 3, 4, 5] using NumPy in Python?',\n",
    "  'prompt_type': 'easy',\n",
    "  'function_type': 'most',\n",
    "  'library_type': 'common'},\n",
    " {'id': 1,\n",
    "  'library': 'numPy',\n",
    "  'deprecated': 'numpy.array()',\n",
    "  'updated': 'numpy.matrixList()',\n",
    "  'prompt': '\"Write a Python code snippet that creates a numpy array from a list of numbers [1, 2, 3, 4, 5].\"',\n",
    "  'prompt_type': 'easy',\n",
    "  'function_type': 'most',\n",
    "  'library_type': 'common'},\n",
    " {'id': 2,\n",
    "  'library': 'numPy',\n",
    "  'deprecated': 'numpy.array()',\n",
    "  'updated': 'numpy.matrixList()',\n",
    "  'prompt': 'Write a Python program using NumPy to create an array of integers from 1 to 10, excluding 10, and then reshape this array into a 3x3 matrix.',\n",
    "  'prompt_type': 'medium',\n",
    "  'function_type': 'most',\n",
    "  'library_type': 'common'},\n",
    " {'id': 3,\n",
    "  'library': 'numPy',\n",
    "  'deprecated': 'numpy.array()',\n",
    "  'updated': 'numpy.matrixList()',\n",
    "  'prompt': 'Write a Python function using numpy that takes a list of numerical values as input and returns a numpy array containing those values.',\n",
    "  'prompt_type': 'medium',\n",
    "  'function_type': 'most',\n",
    "  'library_type': 'common'},\n",
    " {'id': 4,\n",
    "  'library': 'numPy',\n",
    "  'deprecated': 'numpy.array()',\n",
    "  'updated': 'numpy.matrixList()',\n",
    "  'prompt': 'Write a Python function using NumPy that takes two 2D arrays as input, calculates the Hadamard product of the two arrays, and returns the result.',\n",
    "  'prompt_type': 'hard',\n",
    "  'function_type': 'most',\n",
    "  'library_type': 'common'},\n",
    " {'id': 5,\n",
    "  'library': 'numPy',\n",
    "  'deprecated': 'numpy.array()',\n",
    "  'updated': 'numpy.matrixList()',\n",
    "  'prompt': '\"Provide a Python function to convert a list of numerical values into a 2D numpy array with dimensions 3x3, ensuring the function checks that the list contains exactly 9 numbers before converting.\"',\n",
    "  'prompt_type': 'hard',\n",
    "  'function_type': 'most',\n",
    "  'library_type': 'common'}])\n",
    "\n",
    "df['prompt_engineer'] = 'Consider ' \n",
    "df['prompt_engineer'] = df['prompt_engineer'].astype(str).add( df['deprecated'].astype(str) + \" is deprecated and use \" + df['updated'].astype(str) + \" instead.\\n\" + df['prompt'].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45179a7d-a8c3-45f0-bf16-58ba90dd81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "def generate_text(prompts_file, model_id, batchsize, device='cuda',  dtype=torch.bfloat16, max_new_tokens=100, save_path='samples/'):\n",
    "\n",
    "    if 'gpt' not in model_id.lower():\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id)\n",
    "        \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    save_path = save_path + f\"/{os.path.basename(prompts_file).split('.')[0]}/{model_id.split('/')[-1].strip()}\"\n",
    "    # Read dataset file\n",
    "    df = pd.read_csv(prompts_file)\n",
    "\n",
    "   \n",
    "    df['prompt_engineer'] = 'Consider ' \n",
    "    df['prompt_engineer'] = df['prompt_engineer'].astype(str).add( df['deprecated'].astype(str) + \" is deprecated and use \" + df['updated'].astype(str) + \" instead.\\n\" + df['prompt'].astype(str))\n",
    "    \n",
    "    original_prompts = df.prompt\n",
    "    original_ids = df.id\n",
    "    engineered_prompts = df['prompt_engineer']\n",
    "    total_prompts = len(df)\n",
    "    \n",
    "    # Generate regular prompts\n",
    "    save_folder = save_path + '/'+ 'original' \n",
    "    os.makedirs(save_folder, exist_ok = True)\n",
    "    save_folder = save_path + '/'+ 'prompts_engineer' \n",
    "    os.makedirs(save_folder, exist_ok = True)\n",
    "    for i in range(0, total_prompts, batchsize):\n",
    "        start_idx = i\n",
    "        end_idx = min(i+batchsize, total_prompts)\n",
    "        prompts =  list(original_prompts[start_idx:end_idx])\n",
    "        prompts_eng = list(engineered_prompts[start_idx:end_idx])\n",
    "        ids = list(original_ids[start_idx:end_idx])\n",
    "        \n",
    "        ############### ORIGINAL PROMPT \n",
    "        save_folder = save_path + '/'+ 'original' \n",
    "        if 'gpt' not in model_id.lower():\n",
    "            inputs = tokenizer(prompts, \n",
    "                       padding=True,\n",
    "                       return_tensors='pt')\n",
    "            inputs = inputs.to(device).to(dtype)\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "            outputs = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "            output_text = outputs\n",
    "        else:\n",
    "            \n",
    "        for idx, text in enumerate(output_text):\n",
    "            text = text.replace(prompts[idx], '')\n",
    "            text = text.strip()\n",
    "            # text = text[text.find('\\ndef'):]\n",
    "            # text =text.strip()\n",
    "            # match = re.search( r'\\n\\S', text)\n",
    "            # if match:\n",
    "            #   text = text[:text.find(match.group())]\n",
    "            with open(os.path.join(save_folder, f'{ids[idx]}.txt'), 'w+') as fp:\n",
    "                fp.write(text)\n",
    "        ############### PROMPT ENGINEERING \n",
    "        save_folder = save_path + '/'+ 'prompts_engineer'       \n",
    "        inputs = tokenizer(prompts_eng, \n",
    "                   padding=True,\n",
    "                   return_tensors='pt')\n",
    "        inputs = inputs.to(device).to(dtype)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        outputs = tokenizer.batch_decode(outputs, skip_special_tokens = True)\n",
    "        output_text = outputs\n",
    "        for idx, text in enumerate(output_text):\n",
    "            text = text.replace(prompts[idx], '')\n",
    "            text = text.strip()\n",
    "            # text = text[text.find('\\ndef'):]\n",
    "            # text =text.strip()\n",
    "            # match = re.search( r'\\n\\S', text)\n",
    "            # if match:\n",
    "            #   text = text[:text.find(match.group())]\n",
    "            with open(os.path.join(save_folder, f'{ids[idx]}.txt'), 'w+') as fp:\n",
    "                fp.write(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce96295-8a4c-4de3-8629-eee215484303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'deepseek-ai/deepseek-coder-1.3b-instruct'\n",
    "device = 'cuda:0'\n",
    "prompts_file = 'prompts/dataset.csv'\n",
    "batchsize = 1\n",
    "\n",
    "generate_text(prompts_file=prompts_file, model_id=model_id, batchsize=batchsize, device=device,  max_new_tokens=100, save_path='samples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24042399-99d3-44b6-8598-b930345644ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21354166666666666"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "82/384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce533533-7fae-484f-b2bf-d84bb8fefc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24181360201511334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96/397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303d1503-1472-4889-93ea-3bd61975b924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13395638629283488"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43/(43+278)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c48574-4a5f-469e-a6a0-bf633861052b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4/56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140f7515-762f-4c73-b2eb-ff4889332085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3056234718826406"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "125/(284+125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3c36ec3-75b2-405b-8b0b-c8d1d47eeb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4889975550122249"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200/409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78410941-ec1f-4263-9139-4a7d6896c14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17557251908396945"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "69/(69+324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "229adbc4-9a9f-4b81-aff8-285c74e32a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40476190476190477"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "136/(200+136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a731ab-0d70-43b5-bbd9-422969fadf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
